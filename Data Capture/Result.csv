Title,Link,Description
Encoding Musical Style with Transformer Autoencoders. ,http://arxiv.org/abs/1912.05537,"We consider the problem of learning high-level controls over the global
structure of sequence generation, particularly in the context of symbolic music
generation with complex language models. In this work, we present the
Transformer autoencoder, which aggregates encodings of the input data across
time to obtain a global representation of style from a given performance. We
show it is possible to combine this global embedding with other temporally
distributed embeddings, enabling improved control over the separate aspects of
performance style and and melody. Empirically, we demonstrate the effectiveness
of our method on a variety of music generation tasks on the MAESTRO dataset and
a YouTube dataset with 10,000+ hours of piano performances, where we achieve
improvements in terms of log-likelihood and mean listening scores as compared
to relevant baselines.
"
"Taking Ethics, Fairness, and Bias Seriously in Machine Learning for Disaster Risk Management. ",http://arxiv.org/abs/1912.05538,"This paper highlights an important, if under-examined, set of questions about
the deployment of machine learning technologies in the field of disaster risk
management (DRM). While emerging tools show promising capacity to support
scientific efforts to better understand and mitigate the threats posed by
disasters and climate change, our field must undertake a much more careful
assessment of the potential negative impacts that machine learning technologies
may create. We also argue that attention to these issues in the context of
machine learning affords the opportunity to have discussions about potential
ethics, bias, and fairness concerns within disaster data more broadly. In what
follows, we first describe some of the uses and potential benefits of
machine-learning technology in disaster risk management. We then draw on
research from other fields to speculate about potential negative impacts.
Finally, we outline a research agenda for how our disaster risk management can
begin to take these issues seriously and ensure that deployments of
machine-learning tools are conducted in a responsible and beneficial manner.
"
Deep One-bit Compressive Autoencoding. ,http://arxiv.org/abs/1912.05539,"Parameterized mathematical models play a central role in understanding and
design of complex information systems. However, they often cannot take into
account the intricate interactions innate to such systems. On the contrary,
purely data-driven approaches do not need explicit mathematical models for data
generation and have a wider applicability at the cost of interpretability. In
this paper, we consider the design of a one-bit compressive autoencoder, and
propose a novel hybrid model-based and data-driven methodology that allows us
to not only design the sensing matrix for one-bit data acquisition, but also
allows for learning the latent-parameters of an iterative optimization
algorithm specifically designed for the problem of one-bit sparse signal
recovery. Our results demonstrate a significant improvement compared to
state-of-the-art model-based algorithms.
"
Fundamental Entropic Laws and $\mathcal{L}_p$ Limitations of Feedback Systems: Implications for Machine-Learning-in-the-Loop Control. ,http://arxiv.org/abs/1912.05541,"In this paper, we study the fundamental performance limitations for generic
feedback systems in which both the controller and the plant may be arbitrarily
causal while the disturbance can be with any distributions. In particular, we
obtain fundamental $\mathcal{L}_p$ bounds based on the entropic laws that are
inherent in any feedback systems. We also examine the implications of the
generic bounds for machine-learning-in-the-loop control; in other words,
fundamental limits in general exist to what machine learning elements in
feedback loops can achieve.
"
Forging quantum data: classically defeating an IQP-based quantum test. ,http://arxiv.org/abs/1912.05547,"In 2009, Shepherd and Bremner proposed a ""test of quantum capability""
<a href=""/abs/0809.0847"">arXiv:0809.0847</a> that is attractive because the quantum machine's output can be
verified efficiently by classical means. While follow-up papers gave evidence
that directly simulating the quantum prover is classically hard, the security
of the protocol against other (non-simulating) classical attacks has remained
an open question. In this paper, I demonstrate that the protocol is not secure
against classical provers. I describe a classical algorithm that can not only
convince the verifier that the (classical) prover is quantum, but can in fact
can extract the secret key underlying a given protocol instance. Furthermore, I
show that the algorithm is efficient in practice for problem sizes of hundreds
of qubits. Finally, I provide an implementation of the algorithm, and give the
secret vector underlying the ""$25 challenge"" posted online by the authors of
the original paper.
"
Neural Voice Puppetry: Audio-driven Facial Reenactment. ,http://arxiv.org/abs/1912.05566,"We present Neural Voice Puppetry, a novel approach for audio-driven facial
video synthesis. Given an audio sequence of a source person or digital
assistant, we generate a photo-realistic output video of a target person that
is in sync with the audio of the source input. This audio-driven facial
reenactment is driven by a deep neural network that employs a latent 3D face
model space. Through the underlying 3D representation, the model inherently
learns temporal stability while we leverage neural rendering to generate
photo-realistic output frames. Our approach generalizes across different
people, allowing us to synthesize videos of a target actor with the voice of
any unknown source actor or even synthetic voices that can be generated
utilizing standard text-to-speech approaches. Neural Voice Puppetry has a
variety of use-cases, including audio-driven video avatars, video dubbing, and
text-driven video synthesis of a talking head. We demonstrate the capabilities
of our method in a series of audio- and text-based puppetry examples. Our
method is not only more general than existing works since we are generic to the
input person, but we also show superior visual and lip sync quality compared to
photo-realistic audio- and video-driven reenactment techniques.
"
Representation of Federated Learning via Worst-Case Robust Optimization Theory. ,http://arxiv.org/abs/1912.05571,"Federated learning (FL) is a distributed learning approach where a set of
end-user devices participate in the learning process by acting on their
isolated local data sets. Here, we process local data sets of users where
worst-case optimization theory is used to reformulate the FL problem where the
impact of local data sets in training phase is considered as an uncertain
function bounded in a closed uncertainty region. This representation allows us
to compare the performance of FL with its centralized counterpart, and to
replace the uncertain function with a concept of protection functions leading
to more tractable formulation. The latter supports applying a regularization
factor in each user cost function in FL to reach a better performance. We
evaluated our model using the MNIST data set versus the protection function
parameters, e.g., regularization factors.
"
Object Recognition with Human in the Loop Intelligent Frameworks. ,http://arxiv.org/abs/1912.05575,"Classifiers embedded within human in the loop visual object recognition
frameworks commonly utilise two sources of information: one derived directly
from the imagery data of an object, and the other obtained interactively from
user interactions. These computer vision frameworks exploit human high-level
cognitive power to tackle particularly difficult visual object recognition
tasks. In this paper, we present innovative techniques to combine the two
sources of information intelligently for the purpose of improving recognition
accuracy. We firstly employ standard algorithms to build two classifiers for
the two sources independently, and subsequently fuse the outputs from these
classifiers to make a conclusive decision. The two fusion techniques proposed
are: i) a modified naive Bayes algorithm that adaptively selects an individual
classifier's output or combines both to produce a definite answer, and ii) a
neural network based algorithm which feeds the outputs of the two classifiers
to a 4-layer feedforward network to generate a final output. We present
extensive experimental results on 4 challenging visual recognition tasks to
illustrate that the new intelligent techniques consistently outperform
traditional approaches to fusing the two sources of information.
"
The Rise of Multiple Institutional Affiliations. ,http://arxiv.org/abs/1912.05576,"The affiliation to an institution provides prestige and identity to
researchers and determines access to resources and infrastructure. Institutions
in turn seek to affiliate researchers to secure their knowledge and skills,
benefiting the research conducted within these institutions and their position
in national and international rankings. This study documents the phenomenon of
researchers having multiple affiliations and discusses potential causes and
consequences. We analyze affiliation information of 8.5M authors from 40
countries, who published 8.9M scientific articles in 14 disciplines since 1996.
We find that multiple affiliations occur both within countries as well as
across borders, and that more than 60% are within the academic research sector.
The share of authors with multiple affiliations increased substantially over
the past two decades and particularly since the mid-2000s. The increase is
particularly pronounced in countries whose funding structures became more
competitive. The rise of multiple affiliations points to fundamental changes in
the organisation of science and challenges our measurements of where scientific
activity takes place.
"
Peek Inside the Closed World: Evaluating Autoencoder-Based Detection of DDoS to Cloud. ,http://arxiv.org/abs/1912.05590,"Machine-learning-based anomaly detection (ML-based AD) has been successful at
detecting DDoS events in the lab. However published evaluations of ML-based AD
have only had limited data and have not provided insight into why it works. To
address limited evaluation against real-world data, we apply autoencoder, an
existing ML-AD model, to 57 DDoS attack events captured at 5 cloud IPs from a
major cloud provider. To improve our understanding for why ML-based AD works or
not works, we interpret this data with feature attribution and counterfactual
explanation. We show that our version of autoencoders work well overall: our
models capture nearly all malicious flows to 2 of the 4 cloud IPs under attacks
(at least 99.99%) but generate a few false negatives (5% and 9%) for the
remaining 2 IPs. We show that our models maintain near-zero false positives on
benign flows to all 5 IPs. Our interpretation of results shows that our models
identify almost all malicious flows with non-whitelisted (non-WL) destination
ports (99.92%) by learning the full list of benign destination ports from
training data (the normality). Interpretation shows that although our models
learn incomplete normality for protocols and source ports, they still identify
most malicious flows with non-WL protocols and blacklisted (BL) source ports
(100.0% and 97.5%) but risk false positives. Interpretation also shows that our
models only detect a few malicious flows with BL packet sizes (8.5%) by
incorrectly inferring these BL sizes as normal based on incomplete normality
learned. We find our models still detect a quarter of flows (24.7%) with
abnormal payload contents even when they do not see payload by combining
anomalies from multiple flow features. Lastly, we summarize the implications of
what we learn on applying autoencoder-based AD in production.
"
Simultaneous Detection and Removal of Dynamic Objects in Multi-view Images. ,http://arxiv.org/abs/1912.05591,"Consider a set of images of a scene consisting of moving objects captured
using a hand-held camera. In this work, we propose an algorithm which takes
this set of multi-view images as input, detects the dynamic objects present in
the scene, and replaces them with the static regions which are being occluded
by them. The proposed algorithm scans the reference image in the row-major
order at the pixel level and classifies each pixel as static or dynamic. During
the scan, when a pixel is classified as dynamic, the proposed algorithm
replaces that pixel value with the corresponding pixel value of the static
region which is being occluded by that dynamic region. We show that we achieve
artifact-free removal of dynamic objects in multi-view images of several
real-world scenes. To the best of our knowledge, we propose the first method
which simultaneously detects and removes the dynamic objects present in
multi-view images.
"
Practical Sized Typing for Coq. ,http://arxiv.org/abs/1912.05601,"Termination of recursive functions and productivity of corecursive functions
are important for maintaining logical consistency in proof assistants. However,
contemporary proof assistants, such as Coq, rely on syntactic criteria that
prevent users from easily writing obviously terminating or productive programs,
such as quicksort. This is troublesome, since there exist theories for
type-based termination- and productivity-checking. In this paper, we present a
design and implementation of sized type checking and inference for Coq. We
extend past work on sized types for the Calculus of (Co)\-Inductive
Constructions (CIC) with support for global definitions found in Gallina, and
extend the sized-type inference algorithm to support completely unannotated
Gallina terms. This allows our design to maintain complete backward
compatibility with existing Coq developments. We provide an implementation that
extends the Coq kernel with optional support for sized types.
"
A Novel Approach in Strategic Planning of Power Networks Against Physical Attacks. ,http://arxiv.org/abs/1912.05603,"The reported work points at developing a practical approach for power
transmission planners to secure power networks from potential deliberate
attacks. We study the interaction between a system planner (defender) and a
rational attacker who threatens the operation of the power grid. In addition to
the commonly used hardening strategy for protecting the network, a new sort of
resource is introduced under the deception concept. Feint and deception are
acknowledged as effective tools for misleading the attacker in strategic
planning. To this end, the defender deception is mathematically formulated by
releasing misinformation about his plan in the shared cognition-based model. To
reduce the risk of damage in case of deception failure, preemptive-goal
programming is utilized to prioritize the hardening strategy for the vital
components. Furthermore, the value of posturing is introduced which is the
benefits that the deception brings to the system. The problems are formulated
as tri-level mixed-integer linear programming and solved by the
constraint-and-column generation method. Comprehensive simulation studies
performed on WSCC 9-bus and IEEE 118-bus systems indicate how the defender will
save significant cost from protecting his network with posturing rather than
hardening and the proposed approach is a promising development to ensure the
secure operation of power networks.
"
"A Billion Ways to Grasp: An Evaluation of Grasp Sampling Schemes on a Dense, Physics-based Grasp Data Set. ",http://arxiv.org/abs/1912.05604,"Robot grasping is often formulated as a learning problem. With the increasing
speed and quality of physics simulations, generating large-scale grasping data
sets that feed learning algorithms is becoming more and more popular. An often
overlooked question is how to generate the grasps that make up these data sets.
In this paper, we review, classify, and compare different grasp sampling
strategies. Our evaluation is based on a fine-grained discretization of SE(3)
and uses physics-based simulation to evaluate the quality and robustness of the
corresponding parallel-jaw grasps. Specifically, we consider more than 1
billion grasps for each of the 21 objects from the YCB data set. This dense
data set lets us evaluate existing sampling schemes w.r.t. their bias and
efficiency. Our experiments show that some popular sampling schemes contain
significant bias and do not cover all possible ways an object can be grasped.
"
Content Generation for Workforce Training. ,http://arxiv.org/abs/1912.05606,"Efficient workforce training is needed in today's world in which technology
is continually changing the nature of work. Students need to be prepared to
enter the workforce. Employees need to become lifelong learners to stay
up-to-date in their work and to adapt when job functions are eliminated. The
training needs are across all industries - including manufacturing,
construction, and healthcare. Computing systems, in particular
Virtual/Augmented Reality systems, have been adopted in many training
application and show even more promise in the future. However, there are
fundamental limitations in today's systems that limit the domains where
computing systems can be applied and the extent to which they can be deployed.
These limitations need to be addressed by new computing research. In particular
research is needed at multiple levels:
</p>
<p>- Application Data Collection Level Requiring High Security and Privacy
Protections
</p>
<p>- Training Material Authoring Level
</p>
<p>- Software Systems Level
</p>
<p>- Hardware Level
</p>
<p>To accomplish these research goals, a training community needs to be
established to do research in end-to-end training systems and to create a
community of learning and domain experts available for consulting for in depth
computing research on individual system components.
"
Optimization of Integer-Forcing Precoding for Multi-User MIMO Downlink. ,http://arxiv.org/abs/1912.05609,"Integer-forcing (IF) precoding is an alternative to linear precoding for
multi-user (MU) multiple-input-multiple-output (MIMO) channels. IF precoding
can be viewed as a generalization of lattice-reduction-aided (LRA) precoding
where symbol-level detection is replaced by codeword-level decoding, allowing
achievable rate expressions to be derived explicitly. A particular form of IF
precoding that is asymptotically optimal for high SNR is the regularized
diagonally-scaled IF (RDIF) scheme, which is specified by a diagonal scaling
matrix $\mathbf{D}$ and an integer-valued effective channel matrix
$\mathbf{A}$. Optimal values for these parameters have been found
(analytically) only for the case of $K = 2$ users. In this paper, a
low-complexity method is proposed to find good parameters for the RDIF scheme
for $K &gt; 2$ users. The proposed method computes matrix $\mathbf{D}$ by solving
a relaxed optimization problem and then matrix $\mathbf{A}$ by the application
of the LLL algorithm---which, in this particular case, is shown to converge
with a fixed number of iterations, resulting in an overall complexity of
$\mathcal{O}(K^3)$. Simulation results show that the proposed method achieves
1a higher sum rate than choosing $\mathbf{D} = \mathbf{I}$ (as in LRA
precoding) and significantly outperforms traditional linear precoding in all
simulated scenarios1
"
"Applying ANN, ANFIS, and LSSVM Models for Estimation of Acid Solvent Solubility in Supercritical CO$_2$. ",http://arxiv.org/abs/1912.05612,"In the present work, a novel and the robust computational investigation is
carried out to estimate solubility of different acids in supercritical carbon
dioxide. Four different algorithms such as radial basis function artificial
neural network, Multi-layer Perceptron (MLP) artificial neural network (ANN),
Least squares support vector machine (LSSVM) and adaptive neuro-fuzzy inference
system (ANFIS) are developed to predict the solubility of different acids in
carbon dioxide based on the temperature, pressure, hydrogen number, carbon
number, molecular weight, and acid dissociation constant of acid. In the
purpose of best evaluation of proposed models, different graphical and
statistical analyses and also a novel sensitivity analysis are carried out. The
present study proposed the great manners for best acid solubility estimation in
supercritical carbon dioxide, which can be helpful for engineers and chemists
to predict operational conditions in industries.
"
Ensuring Liveness Properties of Distributed Systems: Open Problems. ,http://arxiv.org/abs/1912.05616,"Often fairness assumptions need to be made in order to establish liveness
properties of distributed systems, but in many situations they lead to false
conclusions. This document presents a research agenda aiming at laying the
foundations of a theory of concurrency that is equipped to ensure liveness
properties of distributed systems without making fairness assumptions. This
theory will encompass process algebra, temporal logic and semantic models. The
agenda also includes the development of a methodology and tools that allow
successful application of this theory to the specification, analysis and
verification of realistic distributed systems. Contemporary process algebras
and temporal logics fail to make distinctions between systems of which one has
a crucial liveness property and the other does not, at least when assuming
justness, a strong progress property, but not assuming fairness. Setting up an
alternative framework involves giving up on identifying strongly bisimilar
systems, inventing new induction principles, developing new axiomatic bases for
process algebras and new congruence formats for operational semantics, and
creating matching treatments of time and probability. Even simple systems like
fair schedulers or mutual exclusion protocols cannot be accurately specified in
standard process algebras (or Petri nets) in the absence of fairness
assumptions. Hence the work involves the study of adequate language or model
extensions, and their expressive power.
"
Molecular Generative Model Based On Adversarially Regularized Autoencoder. ,http://arxiv.org/abs/1912.05617,"Deep generative models are attracting great attention as a new promising
approach for molecular design. All models reported so far are based on either
variational autoencoder (VAE) or generative adversarial network (GAN). Here we
propose a new type model based on an adversarially regularized autoencoder
(ARAE). It basically uses latent variables like VAE, but the distribution of
the latent variables is obtained by adversarial training like in GAN. The
latter is intended to avoid both inappropriate approximation of posterior
distribution in VAE and difficulty in handling discrete variables in GAN. Our
benchmark study showed that ARAE indeed outperformed conventional models in
terms of validity, uniqueness, and novelty per generated molecule. We also
demonstrated successful conditional generation of drug-like molecules with ARAE
for both cases of single and multiple properties control. As a potential
real-world application, we could generate EGFR inhibitors sharing the scaffolds
of known active molecules while satisfying drug-like conditions simultaneously.
"
"Judge, Jury & Encryptioner: Exceptional Access with a Fixed Social Cost. ",http://arxiv.org/abs/1912.05620,"We present Judge, Jury and Encryptioner (JJE) an exceptional access scheme
for unlocking devices that does not give unilateral power to any single
authority and places final approval to unlock in the hands of peer devices. Our
scheme, JJE, distributes maintenance of the protocol across a network of
""custodians"" such as courts, government agencies, civil rights watchdogs and
academic institutions. Unlock requests, however, can only be approved by a
randomly selected set of unlock delegates, consisting of other peer devices
that must be physically located to gain access. This requires that law
enforcement expend both human and monetary resources and pay a ""fixed social
cost"" in order to find and request the participation of law abiding citizens in
the unlock process.
</p>
<p>Compared to other proposed exceptional access schemes, we believe that JJE
mitigates the risk of mass surveillance, law enforcement abuse, and
vulnerability to unlawful attackers. We aim to raise the bar set by government
sponsored ""backdoors"" which can be used to covertly unlock any device deemed
suspicious by law enforcement. While we propose a concrete construction, our
primary goal with JJE is to spur discussion on ethical exceptional access
schemes that balance privacy of individuals and law enforcement desires.
</p>
<p>Our scheme transparently reveals the use of exceptional access to device
owners, and to the public, in a way that is clear what are the trade offs in
security and privacy. In order to unlock devices, governments must pay a fixed
social cost that, we believe, can be an effective deterrent to mass
surveillance and abuse.
"
Pre-Training of Deep Bidirectional Protein Sequence Representations with Structural Information. ,http://arxiv.org/abs/1912.05625,"A structure of a protein has a direct impact on its properties and functions.
However, identification of structural similarity directly from amino acid
sequences remains as a challenging problem in computational biology. In this
paper, we introduce a novel BERT-wise pre-training scheme for a protein
sequence representation model called PLUS, which stands for Protein sequence
representations Learned Using Structural information. As natural language
representation models capture syntactic and semantic information of words from
a large unlabeled text corpus, PLUS captures structural information of amino
acids from a large weakly labeled protein database. Since the Transformer
encoder, BERT's original model architecture, has a severe computational
requirement to handle long sequences, we first propose to combine a
bidirectional recurrent neural network with the BERT-wise pre-training scheme.
PLUS is designed to learn protein representations with two pre-training
objectives, i.e., masked language modeling and same family prediction. Then,
the pre-trained model can be fine-tuned for a wide range of tasks without
training randomly initialized task-specific models from scratch. It obtains new
state-of-the-art results on both (1) protein-level and (2) amino-acid-level
tasks, outperforming many task-specific algorithms.
"
Large-scale Kernel Methods and Applications to Lifelong Robot Learning. ,http://arxiv.org/abs/1912.05629,"As the size and richness of available datasets grow larger, the opportunities
for solving increasingly challenging problems with algorithms learning directly
from data grow at the same pace. Consequently, the capability of learning
algorithms to work with large amounts of data has become a crucial scientific
and technological challenge for their practical applicability. Hence, it is no
surprise that large-scale learning is currently drawing plenty of research
effort in the machine learning research community. In this thesis, we focus on
kernel methods, a theoretically sound and effective class of learning
algorithms yielding nonparametric estimators. Kernel methods, in their
classical formulations, are accurate and efficient on datasets of limited size,
but do not scale up in a cost-effective manner. Recent research has shown that
approximate learning algorithms, for instance random subsampling methods like
Nystr\""om and random features, with time-memory-accuracy trade-off mechanisms
are more scalable alternatives. In this thesis, we provide analyses of the
generalization properties and computational requirements of several types of
such approximation schemes. In particular, we expose the tight relationship
between statistics and computations, with the goal of tailoring the accuracy of
the learning process to the available computational resources. Our results are
supported by experimental evidence on large-scale datasets and numerical
simulations. We also study how large-scale learning can be applied to enable
accurate, efficient, and reactive lifelong learning for robotics. In
particular, we propose algorithms allowing robots to learn continuously from
experience and adapt to changes in their operational environment. The proposed
methods are validated on the iCub humanoid robot in addition to other
benchmarks.
"
Discriminative Dimension Reduction based on Mutual Information. ,http://arxiv.org/abs/1912.05631,"The ""curse of dimensionality"" is a well-known problem in pattern recognition.
A widely used approach to tackling the problem is a group of subspace methods,
where the original features are projected onto a new space. The lower
dimensional subspace is then used to approximate the original features for
classification. However, most subspace methods were not originally developed
for classification. We believe that direct adoption of these subspace methods
for pattern classification should not be considered best practice. In this
paper, we present a new information theory based algorithm for selecting
subspaces, which can always result in superior performance over conventional
methods. This paper makes the following main contributions: i) it improves a
common practice widely used by practitioners in the field of pattern
recognition, ii) it develops an information theory based technique for
systematically selecting the subspaces that are discriminative and therefore
are suitable for pattern recognition/classification purposes, iii) it presents
extensive experimental results on a variety of computer vision and pattern
recognition tasks to illustrate that the subspaces selected based on maximum
mutual information criterion will always enhance performance regardless of the
classification techniques used.
"
CineFilter: Unsupervised Filtering for Real Time Autonomous Camera Systems. ,http://arxiv.org/abs/1912.05636,"Learning to mimic the smooth and deliberate camera movement of a human
cameraman is an essential requirement for autonomous camera systems. This paper
presents a novel formulation for online and real-time estimation of smooth
camera trajectories. Many works have focused on global optimization of the
trajectory to produce an offline output. Some recent works have tried to extend
this to the online setting, but lack either in the quality of the camera
trajectories or need large labeled datasets to train their supervised model. We
propose two models, one a convex optimization based approach and another a CNN
based model, both of which can exploit the temporal trends in the camera
behavior. Our model is built in an unsupervised way without any ground truth
trajectories and is robust to noisy outliers. We evaluate our models on two
different settings namely a basketball dataset and a stage performance dataset
and compare against multiple baselines and past approaches. Our models
outperform other methods on quantitative and qualitative metrics and produce
smooth camera trajectories that are motivated by cinematographic principles.
These models can also be easily adopted to run in real-time with a low
computational cost, making them fit for a variety of applications.
"
A recipe for creating ideal hybrid memristive-CMOS neuromorphic computing systems. ,http://arxiv.org/abs/1912.05637,"The development of memristive device technologies has reached a level of
maturity to enable the design of complex and large-scale hybrid memristive-CMOS
neural processing systems. These systems offer promising solutions for
implementing novel in-memory computing architectures for machine learning and
data analysis problems. We argue that they are also ideal building blocks for
the integration in neuromorphic electronic circuits suitable for ultra-low
power brain-inspired sensory processing systems, therefore leading to the
innovative solutions for always-on edge-computing and Internet-of-Things (IoT)
applications. Here we present a recipe for creating such systems based on
design strategies and computing principles inspired by those used in mammalian
brains. We enumerate the specifications and properties of memristive devices
required to support always-on learning in neuromorphic computing systems and to
minimize their power consumption. Finally, we discuss in what cases such
neuromorphic systems can complement conventional processing ones and highlight
the importance of exploiting the physics of both the memristive devices and of
the CMOS circuits interfaced to them.
"
Dynamic State and Parameter Estimation for Natural Gas Networks using Real Pipeline System Data. ,http://arxiv.org/abs/1912.05644,"We present a method for joint state and parameter estimation for natural gas
networks where gas pressures and flows through a network of pipes depend on
time-varying injections, withdrawals, and compression controls. The estimation
is posed as an optimal control problem constrained by coupled partial
differential equations on each pipe that describe space- and time-dependent
density and mass flux. These are discretized and combined with nodal conditions
to give dynamic constraints for posing the estimation as nonlinear least
squares problems. We develop a rapid, scalable computational method for
performing the estimation in the presence of measurement and process noise.
Finally, we evaluate its effectiveness using a data set from a capacity
planning model for an actual pipeline system and a month of time-series data
from its supervisory control and data acquisition (SCADA) system.
"
Bayesian Variational Autoencoders for Unsupervised Out-of-Distribution Detection. ,http://arxiv.org/abs/1912.05651,"Despite their successes, deep neural networks still make unreliable
predictions when faced with test data drawn from a distribution different to
that of the training data, constituting a major problem for AI safety. While
this motivated a recent surge in interest in developing methods to detect such
out-of-distribution (OoD) inputs, a robust solution is still lacking. We
propose a new probabilistic, unsupervised approach to this problem based on a
Bayesian variational autoencoder model, which estimates a full posterior
distribution over the decoder parameters using stochastic gradient Markov chain
Monte Carlo, instead of fitting a point estimate. We describe how
information-theoretic measures based on this posterior can then be used to
detect OoD data both in input space as well as in the model's latent space. The
effectiveness of our approach is empirically demonstrated.
"
Learning Human Objectives by Evaluating Hypothetical Behavior. ,http://arxiv.org/abs/1912.05652,"We seek to align agent behavior with a user's objectives in a reinforcement
learning setting with unknown dynamics, an unknown reward function, and unknown
unsafe states. The user knows the rewards and unsafe states, but querying the
user is expensive. To address this challenge, we propose an algorithm that
safely and interactively learns a model of the user's reward function. We start
with a generative model of initial states and a forward dynamics model trained
on off-policy data. Our method uses these models to synthesize hypothetical
behaviors, asks the user to label the behaviors with rewards, and trains a
neural network to predict the rewards. The key idea is to actively synthesize
the hypothetical behaviors from scratch by maximizing tractable proxies for the
value of information, without interacting with the environment. We call this
method reward query synthesis via trajectory optimization (ReQueST). We
evaluate ReQueST with simulated users on a state-based 2D navigation task and
the image-based Car Racing video game. The results show that ReQueST
significantly outperforms prior methods in learning reward models that transfer
to new environments with different initial state distributions. Moreover,
ReQueST safely trains the reward model to detect unsafe states, and corrects
reward hacking before deploying the agent.
"
deepsing: Generating Sentiment-aware Visual Stories using Cross-modal Music Translation. ,http://arxiv.org/abs/1912.05654,"In this paper we propose a deep learning method for performing
attributed-based music-to-image translation. The proposed method is applied for
synthesizing visual stories according to the sentiment expressed by songs. The
generated images aim to induce the same feelings to the viewers, as the
original song does, reinforcing the primary aim of music, i.e., communicating
feelings. The process of music-to-image translation poses unique challenges,
mainly due to the unstable mapping between the different modalities involved in
this process. In this paper, we employ a trainable cross-modal translation
method to overcome this limitation, leading to the first, to the best of our
knowledge, deep learning method for generating sentiment-aware visual stories.
Various aspects of the proposed method are extensively evaluated and discussed
using different songs.
"
VIBE: Video Inference for Human Body Pose and Shape Estimation. ,http://arxiv.org/abs/1912.05656,"Human motion is fundamental to understanding behavior. Despite progress on
single-image 3D pose and shape estimation, existing video-based
state-of-the-art methods fail to produce accurate and natural motion sequences
due to a lack of ground-truth 3D motion data for training. To address this
problem, we propose Video Inference for Body Pose and Shape Estimation (VIBE),
which makes use of an existing large-scale motion capture dataset (AMASS)
together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty
is an adversarial learning framework that leverages AMASS to discriminate
between real human motions and those produced by our temporal pose and shape
regression networks. We define a temporal network architecture and show that
adversarial training, at the sequence level, produces kinematically plausible
motion sequences without in-the-wild ground-truth 3D labels. We perform
extensive experimentation to analyze the importance of motion and demonstrate
the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving
state-of-the-art performance. Code and pretrained models are available at
https://github.com/mkocabas/VIBE.
"
"Cross-layer Design and SDR Implementation of DSA, Backpressure Routing and Network Coding. ",http://arxiv.org/abs/1912.05658,"A cross-layer cognitive radio system is designed to support unicast and
multicast traffic with integration of dynamic spectrum access (DSA),
backpressure algorithm, and network coding for multi-hop networking. The full
protocol stack that operates with distributed coordination and local
information exchange is implemented with software-defined radios (SDRs) and
assessed in a realistic test and evaluation (T\&amp;E) system based on a network
emulation testbed. Without a common control channel, each SDR performs
neighborhood discovery, spectrum sensing and channel estimation, and executes a
distributed extension of backpressure algorithm that optimizes the spectrum
utility (that represents link rates and traffic congestion) with joint DSA and
routing. The backpressure algorithm is extended to support multicast traffic
with network coding deployed over virtual queues (for multicast destinations).
In addition to full rank decoding at destinations, rank deficient decoding is
also considered to reduce the delay. Cognitive network functionalities are
programmed with GNU Radio and Python modules are developed for different
layers. USRP radios are used as RF front ends. A wireless network T\&amp;E system
is presented to execute emulation tests, where radios communicate with each
other through a wireless network emulator that controls physical channels
according to path loss, fading, and topology effects. Emulation tests are
presented for different topologies to evaluate the throughput, backlog and
energy consumption. Results verify the SDR implementation and the joint effect
of DSA, backpressure routing and network coding under realistic channel and
radio hardware effects.
"
Robust Gabor Networks. ,http://arxiv.org/abs/1912.05661,"This work takes a step towards investigating the benefits of merging
classical vision techniques with deep learning models. Formally, we explore the
effect of replacing the first layers of neural network architectures with
convolutional layers that are based on Gabor filters with learnable parameters.
As a first result, we observe that architectures utilizing Gabor filters as
low-level kernels are capable of preserving test set accuracy of deep
convolutional networks. Therefore, this architectural change exalts their
capabilities in extracting useful low-level features. Furthermore, we observe
that the architectures enhanced with Gabor layers gain advantages in terms of
robustness when compared to the regular models. Additionally, the existence of
a closed mathematical expression for the Gabor kernels allows us to develop an
analytical expression for an upper bound to the Lipschitz constant of the Gabor
layer. This expression allows us to propose a simple regularizer to enhance the
robustness of the network. We conduct extensive experiments with several
architectures and datasets, and show the beneficial effects that the
introduction of Gabor layers has on the robustness of deep convolutional
networks.
"
"Computa\c{c}\~ao Urbana da Teoria \`a Pr\'atica: Fundamentos, Aplica\c{c}\~oes e Desafios. ",http://arxiv.org/abs/1912.05662,"The growing of cities has resulted in innumerable technical and managerial
challenges for public administrators such as energy consumption, pollution,
urban mobility and even supervision of private and public spaces in an
appropriate way. Urban Computing emerges as a promising paradigm to solve such
challenges, through the extraction of knowledge, from a large amount of
heterogeneous data existing in urban space. Moreover, Urban Computing
correlates urban sensing, data management, and analysis to provide services
that have the potential to improve the quality of life of the citizens of large
urban centers. Consider this context, this chapter aims to present the
fundamentals of Urban Computing and the steps necessary to develop an
application in this area. To achieve this goal, the following questions will be
investigated, namely: (i) What are the main research problems of Urban
Computing?; (ii) What are the technological challenges for the implementation
of services in Urban Computing?; (iii) What are the main methodologies used for
the development of services in Urban Computing?; and (iv) What are the
representative applications in this field?
"
Measuring the Reliability of Reinforcement Learning Algorithms. ,http://arxiv.org/abs/1912.05663,"Lack of reliability is a well-known issue for reinforcement learning (RL)
algorithms. This problem has gained increasing attention in recent years, and
efforts to improve it have grown substantially. To aid RL researchers and
production users with the evaluation and improvement of reliability, we propose
a set of metrics that quantitatively measure different aspects of reliability.
In this work, we focus on variability and risk, both during training and after
learning (on a fixed policy). We designed these metrics to be general-purpose,
and we also designed complementary statistical tests to enable rigorous
comparisons on these metrics. In this paper, we first describe the desired
properties of the metrics and their design, the aspects of reliability that
they measure, and their applicability to different scenarios. We then describe
the statistical tests and make additional practical recommendations for
reporting results. The metrics and accompanying statistical tools have been
made available as an open-source library, here:
https://github.com/google-research/rl-reliability-metrics . We apply our
metrics to a set of common RL algorithms and environments, compare them, and
analyze the results.
"
Managing Machine Learning Workflow Components. ,http://arxiv.org/abs/1912.05665,"Machine Learning Workflows~(MLWfs) have become essential and a disruptive
approach in problem-solving over several industries. However, the development
process of MLWfs may be complicated, hard to achieve, time-consuming, and
error-prone. To handle this problem, in this paper, we introduce \emph{machine
learning workflow management}~(MLWfM) as a technique to aid the development and
reuse of MLWfs and their components through three aspects: representation,
execution, and creation. More precisely, we discuss our approach to structure
the MLWfs' components and their metadata to aid retrieval and reuse of
components in new MLWfs. Also, we consider the execution of these components
within a tool. The hybrid knowledge representation, called Hyperknowledge,
frames our methodology, supporting the three MLWfM's aspects. To validate our
approach, we show a practical use case in the Oil \&amp; Gas industry.
"
"Ratatoskr: An open-source framework for in-depth power, performance and area analysis in 3D NoCs. ",http://arxiv.org/abs/1912.05670,"We introduce ratatoskr, an open-source framework for in-depth power,
performance and area (PPA) analysis in NoCs for 3D-integrated and heterogeneous
System-on-Chips (SoCs). It covers all layers of abstraction by providing a NoC
hardware implementation on RT level, a NoC simulator on cycle-accurate level
and an application model on transaction level. By this comprehensive approach,
ratatoskr can provide the following specific PPA analyses: Dynamic power of
links can be measured within 2.4% accuracy of bit-level simulations while
maintaining cycle-accurate simulation speed. Router power is determined from RT
level synthesis combined with cycle-accurate simulations. The performance of
the whole NoC can be measured both via cycle-accurate and RT level simulations.
The performance of individual routers is obtained from RT level including
gate-level verification. The NoC area is calculated from RT level. Despite
these manifold features, ratatoskr offers easy two-step user interaction:
First, a single point-of-entry that allows to set design parameters and second,
PPA reports are generated automatically. For both the input and the output,
different levels of abstraction can be chosen for high-level rapid network
analysis or low-level improvement of architectural details. The synthesize NoC
model reduces up to 32% total router power and 3% router area in comparison to
a conventional standard router. As a forward-thinking and unique feature not
found in other NoC PPA-measurement tools, ratatoskr supports heterogeneous 3D
integration that is one of the most promising integration paradigms for
upcoming SoCs. Thereby, ratatoskr lies the groundwork to design their
communication architectures.
"
Linear Mode Connectivity and the Lottery Ticket Hypothesis. ,http://arxiv.org/abs/1912.05671,"We introduce ""instability analysis,"" a framework for assessing whether the
outcome of optimizing a neural network is robust to SGD noise. It entails
training two copies of a network on different random data orders. If error does
not increase along the linear path between the trained parameters, we say the
network is ""stable."" Instability analysis reveals new properties of neural
networks. For example, standard vision models are initially unstable but become
stable early in training; from then on, the outcome of optimization is
determined up to linear interpolation. We leverage instability analysis to
examine iterative magnitude pruning (IMP), the procedure underlying the lottery
ticket hypothesis. On small vision tasks, IMP finds sparse ""matching
subnetworks"" that can train in isolation from initialization to full accuracy,
but it fails to do so in more challenging settings. We find that IMP
subnetworks are matching only when they are stable. In cases where IMP
subnetworks are unstable at initialization, they become stable and matching
early in training. We augment IMP to rewind subnetworks to their weights early
in training, producing sparse subnetworks of large-scale networks, including
Resnet-50 for ImageNet, that train to full accuracy.
</p>
<p>This submission subsumes <a href=""/abs/1903.01611"">1903.01611</a> (""Stabilizing the Lottery Ticket
Hypothesis"" and ""The Lottery Ticket Hypothesis at Scale"").
"
Biases for Emergent Communication in Multi-agent Reinforcement Learning. ,http://arxiv.org/abs/1912.05676,"We study the problem of emergent communication, in which language arises
because speakers and listeners must communicate information in order to solve
tasks. In temporally extended reinforcement learning domains, it has proved
hard to learn such communication without centralized training of agents, due in
part to a difficult joint exploration problem. We introduce inductive biases
for positive signalling and positive listening, which ease this problem. In a
simple one-step environment, we demonstrate how these biases ease the learning
problem. We also apply our methods to a more extended environment, showing that
agents with these inductive biases achieve better performance, and analyse the
resulting communication protocols.
"
Mobility-Aware Smart Charging of Electric Bus Fleets. ,http://arxiv.org/abs/1912.05681,"We study the joint route assignment and charge scheduling problem of a
transit system dispatcher operating a fleet of electric buses in order to
maximize solar energy integration and reduce energy costs. Specifically, we
consider a complex bus transit system with preexisting routes, limited charging
infrastructure, limited number of electric buses, and time-varying electricity
rates. We present a mixed integer linear program (MILP) that yields the minimal
cost daily operation strategy for the fleet (i.e., route assignments and
charging schedules using daily solar forecasts). We present numerical results
from a real-world case study with Stanford University's Marguerite Shuttle (a
large-scale electric bus fleet) to demonstrate the validity of our solution and
highlight the significant cost savings compared to the status quo.
"
Learning to Model Aspects of Hearing Perception Using Neural Loss Functions. ,http://arxiv.org/abs/1912.05683,"We present a framework to model the perceived quality of audio signals by
combining convolutional architectures, with ideas from classical signal
processing, and describe an approach to enhancing perceived acoustical quality.
We demonstrate the approach by transforming the sound of an inexpensive musical
with degraded sound quality to that of a high-quality musical instrument
without the need for parallel data which is often hard to collect. We adapt the
classical approach of a simple adaptive EQ filtering to the objective criterion
learned by a neural architecture and optimize it to get the signal of our
interest. Since we learn adaptive masks depending on the signal of interest as
opposed to a fixed transformation for all the inputs, we show that shallow
neural architectures can achieve the desired result. A simple constraint on the
objective and the initialization helps us in avoiding adversarial examples,
which otherwise would have produced noisy, unintelligible audio. We believe
that the current framework proposed has enormous applications, in a variety of
problems where one can learn a loss function depending on the problem, using a
neural architecture and optimize it after it has been learned.
"
Online Deep Reinforcement Learning for Autonomous UAV Navigation and Exploration of Outdoor Environments. ,http://arxiv.org/abs/1912.05684,"With the rapidly growing expansion in the use of UAVs, the ability to
autonomously navigate in varying environments and weather conditions remains a
highly desirable but as-of-yet unsolved challenge. In this work, we use Deep
Reinforcement Learning to continuously improve the learning and understanding
of a UAV agent while exploring a partially observable environment, which
simulates the challenges faced in a real-life scenario. Our innovative approach
uses a double state-input strategy that combines the acquired knowledge from
the raw image and a map containing positional information. This positional data
aids the network understanding of where the UAV has been and how far it is from
the target position, while the feature map from the current scene highlights
cluttered areas that are to be avoided. Our approach is extensively tested
using variants of Deep Q-Network adapted to cope with double state input data.
Further, we demonstrate that by altering the reward and the Q-value function,
the agent is capable of consistently outperforming the adapted Deep Q-Network,
Double Deep Q- Network and Deep Recurrent Q-Network. Our results demonstrate
that our proposed Extended Double Deep Q-Network (EDDQN) approach is capable of
navigating through multiple unseen environments and under severe weather
conditions.
"
"Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax. ",http://arxiv.org/abs/1912.05686,"Deep learning models are full of hyperparameters, which are set manually
before the learning process can start. To find the best configuration for these
hyperparameters in such a high dimensional space, with time-consuming and
expensive model training / validation, is not a trivial challenge. Bayesian
optimization is a powerful tool for the joint optimization of hyperparameters,
efficiently trading off exploration and exploitation of the hyperparameter
space. In this paper, we discuss Bayesian hyperparameter optimization,
including hyperparameter optimization, Bayesian optimization, and Gaussian
processes. We also review BoTorch, GPyTorch and Ax, the new open-source
frameworks that we use for Bayesian optimization, Gaussian process inference
and adaptive experimentation, respectively. For experimentation, we apply
Bayesian hyperparameter optimization, for optimizing group weights, to weighted
group pooling, which couples unsupervised tiered graph autoencoders learning
and supervised graph classification learning for molecular graphs. We find that
Ax, BoTorch and GPyTorch together provide a simple-to-use but powerful
framework for Bayesian hyperparameter optimization, using Ax's high-level API
that constructs and runs a full optimization loop and returns the best
hyperparameter configuration.
"
REFINED (REpresentation of Features as Images with NEighborhood Dependencies): A novel feature representation for Convolutional Neural Networks. ,http://arxiv.org/abs/1912.05687,"Deep learning with Convolutional Neural Networks has shown great promise in
various areas of image-based classification and enhancement but is often
unsuitable for predictive modeling involving non-image based features or
features without spatial correlations. We present a novel approach for
representation of high dimensional feature vector in a compact image form,
termed REFINED (REpresentation of Features as Images with NEighborhood
Dependencies), that is conducible for convolutional neural network based deep
learning. We consider the correlations between features to generate a compact
representation of the features in the form of a two-dimensional image using
minimization of pairwise distances similar to multi-dimensional scaling. We
hypothesize that this approach enables embedded feature selection and
integrated with Convolutional Neural Network based Deep Learning can produce
more accurate predictions as compared to Artificial Neural Networks, Random
Forests and Support Vector Regression. We illustrate the superior predictive
performance of the proposed representation, as compared to existing approaches,
using synthetic datasets, cell line efficacy prediction based on drug chemical
descriptors for NCI60 dataset and drug sensitivity prediction based on
transcriptomic data and chemical descriptors using GDSC dataset. Results
illustrated on both synthetic and biological datasets shows the higher
prediction accuracy of the proposed framework as compared to existing
methodologies while maintaining desirable properties in terms of bias and
feature extraction.
"
Learned Variable-Rate Image Compression with Residual Divisive Normalization. ,http://arxiv.org/abs/1912.05688,"Recently it has been shown that deep learning-based image compression has
shown the potential to outperform traditional codecs. However, most existing
methods train multiple networks for multiple bit rates, which increases the
implementation complexity. In this paper, we propose a variable-rate image
compression framework, which employs more Generalized Divisive Normalization
(GDN) layers than previous GDN-based methods. Novel GDN-based residual
sub-networks are also developed in the encoder and decoder networks. Our scheme
also uses a stochastic rounding-based scalable quantization. To further improve
the performance, we encode the residual between the input and the reconstructed
image from the decoder network as an enhancement layer. To enable a single
model to operate with different bit rates and to learn multi-rate image
features, a new objective function is introduced. Experimental results show
that the proposed framework trained with variable-rate objective function
outperforms all standard codecs such as H.265/HEVC-based BPG and
state-of-the-art learning-based variable-rate methods.
"
Tensor Completion for Weakly-dependent Data on Graph for Metro Passenger Flow Prediction. ,http://arxiv.org/abs/1912.05693,"Low-rank tensor decomposition and completion have attracted significant
interest from academia given the ubiquity of tensor data. However, the low-rank
structure is a global property, which will not be fulfilled when the data
presents complex and weak dependencies given specific graph structures. One
particular application that motivates this study is the spatiotemporal data
analysis. As shown in the preliminary study, weakly dependencies can worsen the
low-rank tensor completion performance. In this paper, we propose a novel
low-rank CANDECOMP / PARAFAC (CP) tensor decomposition and completion framework
by introducing the $L_{1}$-norm penalty and Graph Laplacian penalty to model
the weakly dependency on graph. We further propose an efficient optimization
algorithm based on the Block Coordinate Descent for efficient estimation. A
case study based on the metro passenger flow data in Hong Kong is conducted to
demonstrate improved performance over the regular tensor completion methods.
"
Near-optimal Oracle-efficient Algorithms for Stationary and Non-Stationary Stochastic Linear Bandits. ,http://arxiv.org/abs/1912.05695,"We investigate the design of two algorithms that enjoy not only computational
efficiency induced by Hannan's perturbation approach, but also minimax-optimal
regret bounds in linear bandit problems where the learner has access to an
offline optimization oracle. We present an algorithm called
Follow-The-Gaussian-Perturbed Leader (FTGPL) for stationary linear bandit where
each action is associated with a $d$-dimensional feature vector, and prove that
FTGPL (1) achieves the minimax-optimal $\tilde{\mathcal{O}}(d\sqrt{T})$ regret,
(2) matches the empirical performance of Linear Thompson Sampling, and (3) can
be efficiently implemented even in the case of infinite actions, thus achieving
the best of three worlds. Furthermore, it firmly solves an open problem raised
in \citet{abeille2017linear}, which perturbation achieves minimax-optimality in
Linear Thompson Sampling. The weighted variant with exponential discounting,
Discounted Follow-The-Gaussian-Perturbed Leader (D-FTGPL) is proposed to
gracefully adjust to non-stationary environment where unknown parameter is
time-varying within total variation $B_T$. It asymptotically achieves optimal
dynamic regret $\tilde{\mathcal{O}}( d ^{2/3}B_T^{1/3} T^{2/3})$ and is
oracle-efficient due to access to an offline optimization oracle induced by
Gaussian perturbation.
"
What it Thinks is Important is Important: Robustness Transfers through Input Gradients. ,http://arxiv.org/abs/1912.05699,"Adversarial perturbations are imperceptible changes to input pixels that can
change the prediction of deep learning models. Learned weights of models robust
to such perturbations are previously found to be transferable across different
tasks but this applies only if the model architecture for the source and target
tasks is the same. Input gradients characterize how small changes at each input
pixel affect the model output. Using only natural images, we show here that
training a student model's input gradients to match those of a robust teacher
model can gain robustness close to a strong baseline that is robustly trained
from scratch. Through experiments in MNIST, CIFAR-10, CIFAR-100 and
Tiny-ImageNet, we show that our proposed method, input gradient adversarial
matching, can transfer robustness across different tasks and even across
different model architectures. This demonstrates that directly targeting the
semantics of input gradients is a feasible way towards adversarial robustness.
"
Short simplex paths in lattice polytopes. ,http://arxiv.org/abs/1912.05712,"We consider the problem of optimizing a linear function over a lattice
polytope P contained in [0,k]^n and defined via m linear inequalities. We
design a simplex algorithm that, given an initial vertex, reaches an optimal
vertex by tracing a path along the edges of P of length at most O(n^6 k log k).
The length of this path is independent on m and is the best possible up to a
polynomial function, since it is only polynomially far from the worst case
diameter. The number of arithmetic operations needed to compute the next vertex
in the path is polynomial in n, m and log k. If k is polynomially bounded by n
and m, the algorithm runs in strongly polynomial time.
"
A numerical study of the pollution error and DPG adaptivity for long waveguide simulations. ,http://arxiv.org/abs/1912.05716,"High-frequency wave propagation has many important applications in acoustics,
elastodynamics, and electromagnetics. Unfortunately, the finite element
discretization for these problems suffer from significant numerical pollution
errors that increase with the wavenumber. It is critical to control these
errors to obtain a stable and accurate method. We study the effect of pollution
for very long waveguide problems in the context of robust discontinuous
Petrov-Galerkin (DPG) finite element discretizations. Our numerical experiments
show that the pollution primarily has a diffusive effect causing energy loss in
the DPG method while phase errors appear less significant. We report results
for 3D vectorial time-harmonic Maxwell problems in waveguides with more than
8000 wavelengths. Our results corroborate previous analysis for the Galerkin
discretization of the Helmholtz operator by Melenk and Sauter (""Wavenumber
explicit convergence analysis for Galerkin discretizations of the Helmholtz
equation"". In: SIAM J. Numer. Anal. 49.3 (2011), pp. 1210-1243). Additionally,
we discuss adaptive refinement strategies for multi-mode fiber waveguides where
the propagating transverse modes must be resolved sufficiently. Our study shows
the applicability of the DPG error indicator to this class of problems.
Finally, we illustrate the importance of load balancing in these simulations
for distributed-memory parallel computing.
"
Sparse Interpolation With Errors in Chebyshev Basis Beyond Redundant-Block Decoding. ,http://arxiv.org/abs/1912.05719,"We present sparse interpolation algorithms for recovering a polynomial with
$\le B$ terms from $N$ evaluations at distinct values for the variable when
$\le E$ of the evaluations can be erroneous. Our algorithms perform exact
arithmetic in the field of scalars $\mathsf{K}$ and the terms can be standard
powers of the variable or Chebyshev polynomials, in which case the
characteristic of $\mathsf{K}$ is $\ne 2$. Our algorithms return a list of
valid sparse interpolants for the $N$ support points and run in
polynomial-time. For standard power basis our algorithms sample at $N = \lfloor
\frac{4}{3} E + 2 \rfloor B$ points and for Chebyshev basis at $N = \lfloor
\frac{3}{2} E + 2 \rfloor B$ points. Those are fewer points than $N = 2(E+1)B -
1$ in [Kaltofen and Pernet, Proc. ISSAC 2014] for standard power basis and
fewer than in [Arnold and Kaltofen, Proc. ISSAC 2015] for Chebyshev basis,
where only the cases $B\le 3$ have explicit counts for $N$. Our method shows
how to correct $2$ errors in a block of $4B$ points for standard basis and how
to correct $1$ error in a block of $3B$ points for Chebyshev Basis.
"
Using Deep Learning to Solve Computer Security Challenges: A Survey. ,http://arxiv.org/abs/1912.05721,"Although using machine learning techniques to solve computer security
challenges is not a new idea, the rapidly emerging Deep Learning technology has
recently triggered a substantial amount of interests in the computer security
community. This paper seeks to provide a dedicated review of the very recent
research works on using Deep Learning techniques to solve computer security
challenges. In particular, the review covers eight computer security problems
being solved by applications of Deep Learning: security-oriented program
analysis, defending return-oriented programming (ROP) attacks, achieving
control-flow integrity (CFI), defending network attacks, malware
classification, system-event-based anomaly detection, memory forensics, and
fuzzing for software security.
"
On the relationship between multitask neural networks and multitask Gaussian Processes. ,http://arxiv.org/abs/1912.05723,"Despite the effectiveness of multitask deep neural network (MTDNN), there is
a limited theoretical understanding on how the information is shared across
different tasks in MTDNN. In this work, we establish a formal connection
between MTDNN with infinitely-wide hidden layers and multitask Gaussian Process
(GP). We derive multitask GP kernels corresponding to both single-layer and
deep multitask Bayesian neural networks (MTBNN) and show that information among
different tasks is shared primarily due to correlation across last layer
weights of MTBNN and shared hyper-parameters, which is contrary to the popular
hypothesis that information is shared because of shared intermediate layer
weights. Our construction enables using multitask GP to perform efficient
Bayesian inference for the equivalent MTDNN with infinitely-wide hidden layers.
Prior work on the connection between deep neural networks and GP for single
task settings can be seen as special cases of our construction. We also present
an adaptive multitask neural network architecture that corresponds to a
multitask GP with more flexible kernels, such as Linear Model of
Coregionalization (LMC) and Cross-Coregionalization (CC) kernels. We provide
experimental results to further illustrate these ideas on synthetic and real
datasets.
"
Semantic segmentation of trajectories with improved agent models for pedestrian behavior analysis. ,http://arxiv.org/abs/1912.05727,"In this paper, we propose a method for semantic segmentation of pedestrian
trajectories based on pedestrian behavior models, or agents. The agents model
the dynamics of pedestrian movements in two-dimensional space using a linear
dynamics model and common start and goal locations of trajectories. First,
agent models are estimated from the trajectories obtained from image sequences.
Our method is built on top of the Mixture model of Dynamic pedestrian Agents
(MDA); however, the MDA's trajectory modeling and estimation are improved.
Then, the trajectories are divided into semantically meaningful segments. The
subsegments of a trajectory are modeled by applying a hidden Markov model using
the estimated agent models. Experimental results with a real trajectory dataset
show the effectiveness of the proposed method as compared to the well-known
classical Ramer-Douglas-Peucker algorithm and also to the original MDA model.
"
AliMe KBQA: Question Answering over Structured Knowledge for E-commerce Customer Service. ,http://arxiv.org/abs/1912.05728,"With the rise of knowledge graph (KG), question answering over knowledge base
(KBQA) has attracted increasing attention in recent years. Despite much
research has been conducted on this topic, it is still challenging to apply
KBQA technology in industry because business knowledge and real-world questions
can be rather complicated. In this paper, we present AliMe-KBQA, a bold attempt
to apply KBQA in the E-commerce customer service field. To handle real
knowledge and questions, we extend the classic ""subject-predicate-object (SPO)""
structure with property hierarchy, key-value structure and compound value type
(CVT), and enhance traditional KBQA with constraints recognition and reasoning
ability. We launch AliMe-KBQA in the Marketing Promotion scenario for merchants
during the ""Double 11"" period in 2018 and other such promotional events
afterwards. Online results suggest that AliMe-KBQA is not only able to gain
better resolution and improve customer satisfaction, but also becomes the
preferred knowledge management method by business knowledge staffs since it
offers a more convenient and efficient management experience.
"
Improved Activity Forecasting for Generating Trajectories. ,http://arxiv.org/abs/1912.05729,"An efficient inverse reinforcement learning for generating trajectories is
proposed based of 2D and 3D activity forecasting. We modify reward function
with $L_p$ norm and propose convolution into value iteration steps, which is
called convolutional value iteration. Experimental results with seabird
trajectories (43 for training and 10 for test), our method is best in terms of
MHD error and performs fastest. Generated trajectories for interpolating
missing parts of trajectories look much similar to real seabird trajectories
than those by the previous works.
"
Meaning guided video captioning. ,http://arxiv.org/abs/1912.05730,"Current video captioning approaches often suffer from problems of missing
objects in the video to be described, while generating captions semantically
similar with ground truth sentences. In this paper, we propose a new approach
to video captioning that can describe objects detected by object detection, and
generate captions having similar meaning with correct captions. Our model
relies on S2VT, a sequence-to-sequence model for video captioning. Given a
sequence of video frames, the encoding RNN takes a frame as well as detected
objects in the frame in order to incorporate the information of the objects in
the scene. The following decoding RNN outputs are then fed into an attention
layer and then to a decoder for generating captions. The caption is compared
with the ground truth by learning metric so that vector representations of
generated captions are semantically similar to those of ground truth.
Experimental results with the MSDV dataset demonstrate that the performance of
the proposed approach is much better than the model without the proposed
meaning-guided framework, showing the effectiveness of the proposed model. Code
are publicly available at
https://github.com/captanlevi/Meaning-guided-video-captioning-.
"
Robust Data-driven Profile-based Pricing Schemes. ,http://arxiv.org/abs/1912.05731,"To enable an efficient electricity market, a good pricing scheme is of vital
importance. Among many practical schemes, customized pricing is commonly
believed to be able to best exploit the flexibility in the demand side.
However, due to the large volume of consumers in the electricity sector, such
task is simply too overwhelming. In this paper, we first compare two data
driven schemes: one based on load profile and the other based on user's
marginal system cost. Vulnerability analysis shows that the former approach may
lead to loopholes in the electricity market while the latter one is able to
guarantee the robustness, which yields our robust data-driven pricing scheme.
Although k-means clustering is in general NP-hard, surprisingly, by exploiting
the structure of our problem, we design an efficient yet optimal k-means
clustering algorithm to implement our proposed scheme.
"
Fundamental Limits of Lossless Data Compression with Side Information. ,http://arxiv.org/abs/1912.05734,"The problem of lossless data compression with side information available to
both the encoder and the decoder is considered. The finite-blocklength
fundamental limits of the best achievable performance are defined, in two
different versions of the problem: Reference-based compression, when a single
side information string is used repeatedly in compressing different source
messages, and pair-based compression, where a different side information string
is used for each source message. General achievability and converse theorems
are established for arbitrary source-side information pairs, and the optimal
asymptotic behaviour of arbitrary compressors is determined for ergodic
source-side information pairs. A central limit theorem and a law of the
iterated logarithm are proved, describing the inevitable fluctuations of the
second-order asymptotically best possible rate, under appropriate mixing
conditions. An idealized version of Lempel-Ziv coding with side information is
shown to be first- and second-order asymptotically optimal, under the same
conditions. Nonasymptotic normal approximation expansions are proved for the
optimal rate in both the reference-based and pair-based settings, for
memoryless sources. These are stated in terms of explicit, finite-blocklength
bounds, that are tight up to third-order terms. Extensions that go
significantly beyond the class of memoryless sources are obtained. The relevant
source dispersion is identified and its relationship with the conditional
varentropy rate is established. Interestingly, the dispersion is different in
reference-based and pair-based compression, and it is proved that the
reference-based dispersion is in general smaller.
"
Gaussian Conditionally Markov Sequences: Dynamic Models and Representations of Reciprocal and Other Classes. ,http://arxiv.org/abs/1912.05739,"Conditionally Markov (CM) sequences are powerful mathematical tools for
modeling problems. One class of CM sequences is the reciprocal sequence. In
application, we need not only CM dynamic models, but also know how to design
model parameters. Models of two important classes of nonsingular Gaussian (NG)
CM sequences, called $CM_L$ and $CM_F$ models, and a model of the NG reciprocal
sequence, called reciprocal $CM_L$ model, were presented in our previous works
and their applications were discussed. In this paper, these models are studied
in more detail, in particular their parameter design. It is shown that every
reciprocal $CM_L$ model can be induced by a Markov model. Then, parameters of
each reciprocal $CM_L$ model can be obtained from those of the Markov model.
Also, it is shown that a NG $CM_L$ ($CM_F$) sequence can be represented by a
sum of a NG Markov sequence and an uncorrelated NG vector. This (necessary and
sufficient) representation provides a basis for designing parameters of a
$CM_L$ ($CM_F$) model. From the CM viewpoint, a representation is also obtained
for NG reciprocal sequences. This representation is simple and reveals an
important property of reciprocal sequences. As a result, the significance of
studying reciprocal sequences from the CM viewpoint is demonstrated. A full
spectrum of dynamic models from a $CM_L$ model to a reciprocal $CM_L$ model is
also presented. Some examples are presented for illustration.
"
The Metagenomic Binning Problem: Clustering Markov Sequences. ,http://arxiv.org/abs/1912.05741,"The goal of metagenomics is to study the composition of microbial
communities, typically using high-throughput shotgun sequencing. In the
metagenomic binning problem, we observe random substrings (called contigs) from
a mixture of genomes and want to cluster them according to their genome of
origin. Based on the empirical observation that genomes of different bacterial
species can be distinguished based on their tetranucleotide frequencies, we
model this task as the problem of clustering N sequences generated by M
distinct Markov processes, where M&lt;&lt;N. Utilizing the large-deviation principle
for Markov processes, we establish the information-theoretic limit for perfect
binning. Specifically, we show that the length of the contigs must scale with
the inverse of the Chernoff Information between the two most similar species.
Our result also implies that contigs should be binned using the conditional
relative entropy as a measure of distance, as opposed to the Euclidean distance
often used in practice.
"
Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep RL. ,http://arxiv.org/abs/1912.05743,"Saliency maps have been used to support explanations of deep reinforcement
learning (RL) agent behavior over temporally extended sequences. However, their
use in the community indicates that the explanations derived from saliency maps
are often unfalsifiable and can be highly subjective. We introduce an empirical
approach grounded in counterfactual reasoning to test the hypotheses generated
from saliency maps and assess the degree to which saliency maps represent
semantics of RL environments. We evaluate three types of saliency maps using
Atari games, a common benchmark for deep RL. Our results show the extent to
which existing claims about Atari games can be evaluated and suggest that
saliency maps are an exploratory tool not an explanatory tool.
"
Artificial Intelligence-Enabled Intelligent 6G Networks. ,http://arxiv.org/abs/1912.05744,"With the rapid development of smart terminals and infrastructures, as well as
diversified applications (e.g., virtual and augmented reality, remote surgery
and holographic projection) with colorful requirements, current networks (e.g.,
4G and upcoming 5G networks) may not be able to completely meet quickly rising
traffic demands. Accordingly, efforts from both industry and academia have
already been put to the research on 6G networks. Recently, artificial
intelligence (AI) has been utilized as a new paradigm for the design and
optimization of 6G networks with a high level of intelligence. Therefore, this
article proposes an AI-enabled intelligent architecture for 6G networks to
realize knowledge discovery, smart resource management, automatic network
adjustment and intelligent service provisioning, where the architecture is
divided into four layers: intelligent sensing layer, data mining and analytics
layer, intelligent control layer and smart application layer. We then review
and discuss the applications of AI techniques for 6G networks and elaborate how
to employ the AI techniques to efficiently and effectively optimize the network
performance, including AI-empowered mobile edge computing, intelligent mobility
and handover management, and smart spectrum management. Moreover, we highlight
important future research directions and potential solutions for AI-enabled
intelligent 6G networks, including computation efficiency, algorithms
robustness, hardware development and energy management.
"
Multi-Agent Task Allocation in Complementary Teams: A Hunter and Gatherer Approach. ,http://arxiv.org/abs/1912.05748,"Consider a dynamic task allocation problem, where tasks are unknowingly
distributed over an environment. This paper considers each task comprised of
two sequential subtasks: detection and completion, where each subtask can only
be carried out by a certain type of agent. We address this problem using a
novel nature-inspired approach called ""hunter and gatherer"". The proposed
method employs two complementary teams of agents: one agile in detecting
(hunters) and another dexterous in completing (gatherers) the tasks. To
minimize the collective cost of task accomplishments in a distributed manner, a
game-theoretic solution is introduced to couple agents from complementary
teams. We utilize market-based negotiation models to develop incentive-based
decision-making algorithms relying on innovative notions of ""certainty and
uncertainty profit margins"". The simulation results demonstrate that employing
two complementary teams of hunters and gatherers can effectually improve the
number of tasks completed by agents compared to conventional methods, while the
collective cost of accomplishments is minimized. In addition, the stability and
efficacy of the proposed solutions are studied using Nash equilibrium analysis
and statistical analysis respectively. It is also numerically shown that the
proposed solutions function fairly, i.e. for each type of agent, the overall
workload is distributed equally.
"
"The Use of Deep Learning for Symbolic Integration: A Review of (Lample and Charton, 2019). ",http://arxiv.org/abs/1912.05752,"Lample and Charton (2019) describe a system that uses deep learning
technology to compute symbolic, indefinite integrals, and to find symbolic
solutions to first- and second-order ordinary differential equations, when the
solutions are elementary functions. They found that, over a particular test
set,the system could find solutions more successfully than sophisticated
packages for symbolic mathematics such as Mathematica run with a long time-out.
However, some important categories of examples are not included in their corpus
and have not been tested. Some of these categories are certainly outside the
scope of their system. Overall their system is entirely dependent on
pre-existing, sophisticated, software for symbolic mathematics; it does not
constitute any kind of triumph of deep learning methods over symbolic methods.
"
Pathway Activity Analysis and Metabolite Annotation for Untargeted Metabolomics using Probabilistic Modeling. ,http://arxiv.org/abs/1912.05753,"Motivation: Untargeted metabolomics comprehensively characterizes small
molecules and elucidates activities of biochemical pathways within a biological
sample. Despite computational advances, interpreting collected measurements and
determining their biological role remains a challenge. Results: To interpret
measurements, we present an inference-based approach, termed Probabilistic
modeling for Untargeted Metabolomics Analysis (PUMA). Our approach captures
measurements and known information about the sample under study in a generative
model and uses stochastic sampling to compute posterior probability
distributions. PUMA predicts the likelihood of pathways being active, and then
derives a probabilistic annotation, which assigns chemical identities to the
measurements. PUMA is validated on synthetic datasets. When applied to test
cases, the resulting pathway activities are biologically meaningful and
distinctly different from those obtained using statistical pathway enrichment
techniques. Annotation results are in agreement to those obtained using other
tools that utilize additional information in the form of spectral signatures.
Importantly, PUMA annotates many additional measurements.
"
Estimating 3D Camera Pose from 2D Pedestrian Trajectories. ,http://arxiv.org/abs/1912.05758,"We consider the task of re-calibrating the 3D pose of a static surveillance
camera, whose pose may change due to external forces, such as birds, wind,
falling objects or earthquakes. Conventionally, camera pose estimation can be
solved with a PnP (Perspective-n-Point) method using 2D-to-3D feature
correspondences, when 3D points are known. However, 3D point annotations are
not always available or practical to obtain in real-world applications. We
propose an alternative strategy for extracting 3D information to solve for
camera pose by using pedestrian trajectories. We observe that 2D pedestrian
trajectories indirectly contain useful 3D information that can be used for
inferring camera pose. To leverage this information, we propose a data-driven
approach by training a neural network (NN) regressor to model a direct mapping
from 2D pedestrian trajectories projected on the image plane to 3D camera pose.
We demonstrate that our regressor trained only on synthetic data can be
directly applied to real data, thus eliminating the need to label any real
data. We evaluate our method across six different scenes from the Town Centre
Street and DUKEMTMC datasets. Our method achieves an average location error of
$0.22m$ and orientation error of $1.97^\circ$.
"
GPRInvNet: Deep Learning-Based Ground Penetrating Radar Data Inversion for Tunnel Lining. ,http://arxiv.org/abs/1912.05759,"A DNN architecture called GPRInvNet is proposed to tackle the challenge of
mapping Ground Penetrating Radar (GPR) B-Scan data to complex permittivity maps
of subsurface structure. GPRInvNet consists of a trace-to-trace encoder and a
decoder. It is specially designed to take account of the characteristics of GPR
inversion when faced with complex GPR B-Scan data as well as addressing the
spatial alignment issue between time-series B-Scan data and spatial
permittivity maps. It fuses features from several adjacent traces on the B-Scan
data to enhance each trace, and then further condense the features of each
trace separately. The sensitive zone on the permittivity map spatially aligned
to the enhanced trace is reconstructed accurately. GPRInvNet has been utilized
to reconstruct the permittivity map of tunnel linings. A diverse range of
dielectric models of tunnel lining containing complex defects has been
reconstructed using GPRInvNet, and results demonstrate that GPRInvNet is
capable of effectively reconstructing complex tunnel lining defects with clear
boundaries. Comparative results with existing baseline methods also demonstrate
the superiority of the GPRInvNet. To generalize GPRInvNet to real GPR data, we
integrated background noise patches recorded form a practical model testing
into synthetic GPR data to train GPRInvNet. The model testing has been
conducted for validation, and experimental results show that GPRInvNet achieves
satisfactory results on real data.
"
IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks. ,http://arxiv.org/abs/1912.05763,"Retinal vessel segmentation is of great interest for diagnosis of retinal
vascular diseases. To further improve the performance of vessel segmentation,
we propose IterNet, a new model based on UNet, with the ability to find
obscured details of the vessel from the segmented vessel image itself, rather
than the raw input image. IterNet consists of multiple iterations of a
mini-UNet, which can be 4$\times$ deeper than the common UNet. IterNet also
adopts the weight-sharing and skip-connection features to facilitate training;
therefore, even with such a large architecture, IterNet can still learn from
merely 10$\sim$20 labeled images, without pre-training or any prior knowledge.
IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream
datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are
the best scores in the literature. The source code is available.
"
CCCNet: An Attention Based Deep Learning Framework for Categorized Crowd Counting. ,http://arxiv.org/abs/1912.05765,"Crowd counting problem that counts the number of people in an image has been
extensively studied in recent years. In this paper, we introduce a new variant
of crowd counting problem, namely ""Categorized Crowd Counting"", that counts the
number of people sitting and standing in a given image. Categorized crowd
counting has many real-world applications such as crowd monitoring, customer
service, and resource management. The major challenges in categorized crowd
counting come from high occlusion, perspective distortion and the seemingly
identical upper body posture of sitting and standing persons. Existing density
map based approaches perform well to approximate a large crowd, but lose
important local information necessary for categorization. On the other hand,
traditional detection-based approaches perform poorly in occluded environments,
especially when the crowd size gets bigger. Hence, to solve the categorized
crowd counting problem, we develop a novel attention-based deep learning
framework that addresses the above limitations. In particular, our approach
works in three phases: i) We first generate basic detection based sitting and
standing density maps to capture the local information; ii) Then, we generate a
crowd counting based density map as global counting feature; iii) Finally, we
have a cross-branch segregating refinement phase that splits the crowd density
map into final sitting and standing density maps using attention mechanism.
Extensive experiments show the efficacy of our approach in solving the
categorized crowd counting problem.
"
One Framework to Register Them All: PointNet Encoding for Point Cloud Alignment. ,http://arxiv.org/abs/1912.05766,"PointNet has recently emerged as a popular representation for unstructured
point cloud data, allowing application of deep learning to tasks such as object
detection, segmentation and shape completion. However, recent works in
literature have shown the sensitivity of the PointNet representation to pose
misalignment. This paper presents a novel framework that uses PointNet encoding
to align point clouds and perform registration for applications such as 3D
reconstruction, tracking and pose estimation. We develop a framework that
compares PointNet features of template and source point clouds to find the
transformation that aligns them accurately. In doing so, we avoid
computationally expensive correspondence finding steps, that are central to
popular registration methods such as ICP and its variants. Depending on the
prior information about the shape of the object formed by the point clouds, our
framework can produce approaches that are shape specific or general to unseen
shapes. Our framework produces approaches that are robust to noise and initial
misalignment in data and work robustly with sparse as well as partial point
clouds. We perform extensive simulation and real-world experiments to validate
the efficacy of our approach and compare the performance with state-of-art
approaches. Code is available at
https://github.com/vinits5/pointnet-registrationframework.
"
Algorithmic Price Discrimination. ,http://arxiv.org/abs/1912.05770,"We consider a generalization of the third degree price discrimination problem
studied in Bergemann et al. (2015), where an intermediary between the buyer and
the seller can design market segments to maximize any linear combination of
consumer surplus and seller revenue. Unlike in Bergemann et al. (2015), we
assume that the intermediary only has partial information about the buyer's
value. We consider three different models of information, with increasing order
of difficulty. In the first model, we assume that the intermediary's
information allows him to construct a probability distribution of the buyer's
value. Next we consider the sample complexity model, where we assume that the
intermediary only sees samples from this distribution. Finally, we consider a
bandit online learning model, where the intermediary can only observe past
purchasing decisions of the buyer, rather than her exact value. For each of
these models, we present algorithms to compute optimal or near optimal market
segmentation.
"
Envisioning Device-to-Device Communications in 6G. ,http://arxiv.org/abs/1912.05771,"To fulfill the requirements of various emerging applications, the future
sixth generation (6G) mobile network is expected to be an innately intelligent,
highly dynamic, ultradense heterogeneous network that interconnects all things
with extremely low-latency and high speed data transmission. It is believed
that artificial intelligence (AI) will be the most innovative technique that
can achieve intelligent automated network operations, management and
maintenance in future complex 6G networks. Driven by AI techniques,
device-to-device (D2D) communication will be one of the pieces of the 6G jigsaw
puzzle. To construct an efficient implementation of intelligent D2D in future
6G, we outline a number of potential D2D solutions associating with 6G in terms
of mobile edge computing, network slicing, and Non-orthogonal multiple access
(NOMA) cognitive Networking.
"
Towards Expressive Priors for Bayesian Neural Networks: Poisson Process Radial Basis Function Networks. ,http://arxiv.org/abs/1912.05779,"While Bayesian neural networks have many appealing characteristics, current
priors do not easily allow users to specify basic properties such as expected
lengthscale or amplitude variance. In this work, we introduce Poisson Process
Radial Basis Function Networks, a novel prior that is able to encode amplitude
stationarity and input-dependent lengthscale. We prove that our novel
formulation allows for a decoupled specification of these properties, and that
the estimated regression function is consistent as the number of observations
tends to infinity. We demonstrate its behavior on synthetic and real examples.
"
CLOSURE: Assessing Systematic Generalization of CLEVR Models. ,http://arxiv.org/abs/1912.05783,"The CLEVR dataset of natural-looking questions about 3D-rendered scenes has
recently received much attention from the research community. A number of
models have been proposed for this task, many of which achieved very high
accuracies of around 97-99%. In this work, we study how systematic the
generalization of such models is, that is to which extent they are capable of
handling novel combinations of known linguistic constructs. To this end, we
test models' understanding of referring expressions based on matching object
properties (such as e.g. ""the object that is the same size as the red ball"") in
novel contexts. Our experiments on the thereby constructed CLOSURE benchmark
show that state-of-the-art models often do not exhibit systematicity after
being trained on CLEVR. Surprisingly, we find that an explicitly compositional
Neural Module Network model also generalizes badly on CLOSURE, even when it has
access to the ground-truth programs at test time. We improve the NMN's
systematic generalization by developing a novel Vector-NMN module architecture
with vector-valued inputs and outputs. Lastly, we investigate the extent to
which few-shot transfer learning can help models that are pretrained on CLEVR
to adapt to CLOSURE. Our few-shot learning experiments contrast the adaptation
behavior of the models with intermediate discrete programs with that of the
end-to-end continuous models.
"
Learning Improvement Heuristics for Solving the Travelling Salesman Problem. ,http://arxiv.org/abs/1912.05784,"Recent studies in using deep learning to solve the Travelling Salesman
Problem (TSP) focus on construction heuristics, the solution of which may still
be far from optimality. To improve solution quality, additional procedures such
as sampling or beam search are required. However, they are still based on the
same construction policy, which is less effective in refining a solution. In
this paper, we propose to directly learn the improvement heuristics for solving
TSP based on deep reinforcement learning.We first present a reinforcement
learning formulation for the improvement heuristic, where the policy guides
selection of the next solution. Then, we propose a deep architecture as the
policy network based on self-attention. Extensive experiments show that,
improvement policies learned by our approach yield better results than
state-of-the-art methods, even from random initial solutions. Moreover, the
learned policies are more effective than the traditional hand-crafted ones, and
robust to different initial solutions with either high or poor quality.
"
Functional a posteriori error estimates for boundary element methods. ,http://arxiv.org/abs/1912.05789,"Functional error estimates are well-estabilished tools for a posteriori error
estimation and related adaptive mesh-refinement for the finite element method
(FEM). The present work proposes a first functional error estimate for the
boundary element method (BEM). One key feature is that the derived error
estimates are independent of the BEM discretization and provide guaranteed
lower and upper bounds for the unknown error. In particular, our analysis
covers Galerkin BEM as well as collocation, which makes our approach of
particular interest for people working in engineering. Numerical experiments
for the Laplace problem confirm the theoretical results.
"
Zooming into Face Forensics: A Pixel-level Analysis. ,http://arxiv.org/abs/1912.05790,"The stunning progress in face manipulation methods has made it possible to
synthesize realistic fake face images, which poses potential threats to our
society. It is urgent to have face forensics techniques to distinguish those
tampered images. A large scale dataset ""FaceForensics++"" has provided enormous
training data generated from prominent face manipulation methods to facilitate
anti-fake research. However, previous works focus more on casting it as a
classification problem by only considering a global prediction. Through
investigation to the problem, we find that training a classification network
often fails to capture high quality features, which might lead to sub-optimal
solutions. In this paper, we zoom in on the problem by conducting a pixel-level
analysis, i.e. formulating it as a pixel-level segmentation task. By evaluating
multiple architectures on both segmentation and classification tasks, We show
the superiority of viewing the problem from a segmentation perspective.
Different ablation studies are also performed to investigate what makes an
effective and efficient anti-fake model. Strong baselines are also established,
which, we hope, could shed some light on the field of face forensics.
"
The Extended HOA Format for Synthesis. ,http://arxiv.org/abs/1912.05793,"We propose a small extension to the Hanoi Omega-Automata format to define
reactive-synthesis problems. Namely, we add a ""controllable-AP"" header item
specifying the subset of atomic propositions which is controllable. We describe
the semantics of the new format and propose an output format for synthesized
strategies. Finally, we also comment on tool support meant to encourage fast
adoption of the extended Hanoi Omega-Automata format for synthesis.
"
Automatic Layout Generation with Applications in Machine Learning Engine Evaluation. ,http://arxiv.org/abs/1912.05796,"Machine learning-based lithography hotspot detection has been deeply studied
recently, from varies feature extraction techniques to efficient learning
models. It has been observed that such machine learning-based frameworks are
providing satisfactory metal layer hotspot prediction results on known public
metal layer benchmarks. In this work, we seek to evaluate how these machine
learning-based hotspot detectors generalize to complicated patterns. We first
introduce a automatic layout generation tool that can synthesize varies layout
patterns given a set of design rules. The tool currently supports both metal
layer and via layer generation. As a case study, we conduct hotspot detection
on the generated via layer layouts with representative machine learning-based
hotspot detectors, which shows that continuous study on model robustness and
generality is necessary to prototype and integrate the learning engines in DFM
flows. The source code of the layout generation tool will be available at
https://github. com/phdyang007/layout-generation.
"
Single-Stage Regulated Resonant WPT Receiver with Low Harmonic Distortion AC Voltage and Continuous DC Current. ,http://arxiv.org/abs/1912.05809,"Resonant rectifier topologies would be a promising candidate for achieving
simple, compact, and reliable single-stage wireless power transfer (WPT)
receiver if not for the lack of good DC regulation capability. This paper
investigates the problems that prevent the feasibility of single-stage DC
regulation in resonant rectifier topologies. A possible solution is the
proposed differential resonant rectifier topology, of which the rectifier is
designed to have a relatively constant AC voltage, and that phase shift control
is used to achieve relatively good output regulation. Design considerations on
the reactive component sizing, magnetic component design, frequency and phase
synchronization, small signal modelling, and closed-loop feedback control
design, are discussed. Experimental results verified that the proposed WPT
receiver system can achieve single-stage AC rectification and DC regulation
while attaining the key features of low harmonic distortion in its AC output
voltage, continuous DC current, and zero-voltage-switching (ZVS) operation over
a wide operating range.
"
An Integral Representation of the Logarithmic Function with Applications in Information Theory. ,http://arxiv.org/abs/1912.05812,"We explore a well-known integral representation of the logarithmic function,
and demonstrate its usefulness in obtaining compact, easily-computable exact
formulas for quantities that involve expectations and higher moments of the
logarithm of a positive random variable (or the logarithm of a sum of such
random variables). The integral representation of the logarithm is proved
useful in a variety of information-theoretic applications, including universal
lossless data compression, entropy and differential entropy evaluations, and
the calculation of the ergodic capacity of the single-input, multiple-output
(SIMO) Gaussian channel with random parameters (known to both transmitter and
receiver). This integral representation and its variants are anticipated to
serve as a useful tool in additional applications, as a rigorous alternative to
the popular (but non-rigorous) replica method (at least in some situations).
"
Single-Switch-Regulated Resonant WPT Receiver. ,http://arxiv.org/abs/1912.05814,"A single-switch-regulated wireless power transfer (WPT) receiver is presented
in this letter. Aiming at low-cost applications, the system involves only a
single-switch class-E resonant rectifier, a frequency synchronization circuit,
and a microcontroller. The number of power semiconductor devices required in
this circuit is minimal. Only one active switch is used and no diode is
required. As a single-switch solution, this simplifies circuit implementation,
improves reliability, and lowers hardware cost. The single-switch resonant
rectifier provides a relatively constant quasi-sinusoidal voltage waveform to
pick up the wireless power from the receiver coil. Due to the resonant nature
of the rectifier, ZVS turn on and turn off are achieved. The steady-state
analysis and discussions on the component sizing and the control design are
provided. A prototype is built and experimental works are performed to verify
the features.
"
On depth spectra of constacyclic codes over finite commutative chain rings. ,http://arxiv.org/abs/1912.05815,"The depth of a sequence plays an important role in studying its linear
complexity in game theory, communication theory and cryptography. In this
paper, we determine depth spectra of all repeated-root $(\alpha+\gamma
\beta)$-constacyclic codes of arbitrary lengths over a finite commutative chain
ring $\mathcal{R},$ where $\alpha$ is a non-zero element of the Teichm\""{u}ller
set of $\mathcal{R}$ and $\beta$ is a unit in $\mathcal{R}.$ We also illustrate
our results with some examples.
"
The Lexicographic Method for the Threshold Cover Problem. ,http://arxiv.org/abs/1912.05819,"The lexicographic method is a technique that was introduced by Hell and Huang
[Journal of Graph Theory, 20(3):361--374, 1995] as a way to simplify the
problems of recognizing and obtaining representations of comparability graphs,
proper circular-arc graphs and proper interval graphs. This method gives rise
to conceptually simple recognition algorithms and leads to much simpler proofs
for some characterization theorems for these classes. Threshold graphs are a
class of graphs that have many equivalent definitions and have applications in
integer programming and set packing problems. A graph is said to have a
threshold cover of size $k$ if its edges can be covered using $k$ threshold
graphs. Chv\'atal and Hammer conjectured in 1977 that given a graph $G$, a
suitably constructed auxiliary graph $G'$ has chromatic number equal to the
minimum size of a threshold cover of $G$. Although Cozzens and Leibowitz showed
that this conjecture is false in the general case, Raschle and Simon
[Proceedings of the Twenty-seventh Annual ACM Symposium on Theory of Computing,
STOC '95, pages 650--661, 1995] proved that $G$ has a threshold cover of size 2
if and only if $G'$ is bipartite. We show how the lexicographic method can be
used to obtain a completely new and much simpler proof for this result. This
method also gives rise to a simple new LexBFS-based algorithm for recognizing
graphs having a threshold cover of size 2. Although this algorithm is not the
fastest known, it is a certifying algorithm that matches the time complexity of
the fastest known certifying algorithm for this problem. The algorithm can also
be easily adapted to give a certifying recognition algorithm for bipartite
graphs that can be covered by two chain subgraphs.
"
A Reconstructed Discontinuous Approximation to Monge-Ampere Equation in Least Square Formation. ,http://arxiv.org/abs/1912.05822,"We propose a numerical method to solve the Monge-Ampere equation which admits
a classical convex solution. The Monge-Ampere equation is reformulated into an
equivalent first-order system. We adopt a novel reconstructed discontinuous
approximation space which consists of piecewise irrotational polynomials. This
space allows us to solve the first order system in two sequential steps. In the
first step, we solve a nonlinear system to obtain the approximation to the
gradient. A Newton iteration is adopted to handle the nonlinearity in the
system. The approximation to the primitive variable is obtained from the
approximate gradient by a trivial least squares finite element method in the
second step. Numerical examples in both two dimensions and three dimensions are
presented to show an optimal convergence rate in accuracy. It is interesting to
observe that the approximate solution is piecewise convex in each element.
Particularly, with the reconstructed approximation space, the proposed method
demonstrates a remarkable robustness. The convergence of the Newton iteration
does not rely on the initial values, which shows a very different behaviour
from references. The dependence of the convergence on the penalty parameter in
the discretization is also negligible, in comparison to the classical
discontinuous approximation space.
"
Smart Contract Repair. ,http://arxiv.org/abs/1912.05823,"Smart contracts are automated or self-enforcing contracts that can be used to
exchange money, property, or anything of value without having to place trust in
third parties. Many commercial transactions presently make use of smart
contracts due to their potential benefits in enabling parties to engage in
secure peer-to-peer transactions independent of external parties. They do so by
transferring trust to computer programs (smart contracts), raising the question
of whether these programs can be fully trusted. However, the code can be
complex and may behave in many different unexpected or malicious ways due to
poorly written or vulnerable smart contracts. Furthermore, in the case of smart
contracts on the blockchain, they are typically open to (malicious) agents
which can interact with it in various ways. Experience shows that many commonly
used smart contracts are vulnerable to serious malicious attacks which may
enable attackers to steal valuable assets of involved parties. There is
therefore a need to apply analysis techniques to detect and repair bugs in
smart contracts before being deployed. In this work, we present the first
automated smart contracts repair approach that is gas-optimized and
vulnerability-agnostic. Our repair method is search-based and considers the gas
usage of the candidate patches via leveraging our novel notation of \emph{gas
dominance relationship}. Our approach can be used to optimise the overall
security and reliability of smart contracts against malicious attackers.
"
Efficient Approximation of the Matching Distance for 2-parameter persistence. ,http://arxiv.org/abs/1912.05826,"The matching distance is a computationally tractable topological measure to
compare multi-filtered simplicial complexes. We design efficient algorithms for
approximating the matching distance of two bi-filtered complexes to any desired
precision $\epsilon&gt;0$. Our approach is based on a quad-tree refinement
strategy introduced by Biasotti et al., but we recast their approach entirely
in geometric terms. This point of view leads to several novel observations
resulting in a practically faster algorithm. We demonstrate this speed-up by
experimental comparison and provide our code in a public repository which
provides the first efficient publicly available implementation of the matching
distance.
"
An Efficient Explorative Sampling Considering the Generative Boundaries of Deep Generative Neural Networks. ,http://arxiv.org/abs/1912.05827,"Deep generative neural networks (DGNNs) have achieved realistic and
high-quality data generation. In particular, the adversarial training scheme
has been applied to many DGNNs and has exhibited powerful performance. Despite
of recent advances in generative networks, identifying the image generation
mechanism still remains challenging. In this paper, we present an explorative
sampling algorithm to analyze generation mechanism of DGNNs. Our method
efficiently obtains samples with identical attributes from a query image in a
perspective of the trained model. We define generative boundaries which
determine the activation of nodes in the internal layer and probe inside the
model with this information. To handle a large number of boundaries, we obtain
the essential set of boundaries using optimization. By gathering samples within
the region surrounded by generative boundaries, we can empirically reveal the
characteristics of the internal layers of DGNNs. We also demonstrate that our
algorithm can find more homogeneous, the model specific samples compared to the
variations of {\epsilon}-based sampling method.
"
Formal Verification of Debates in Argumentation Theory. ,http://arxiv.org/abs/1912.05828,"Humans engage in informal debates on a daily basis. By expressing their
opinions and ideas in an argumentative fashion, they are able to gain a deeper
understanding of a given problem and in some cases, find the best possible
course of actions towards resolving it. In this paper, we develop a methodology
to verify debates formalised as abstract argumentation frameworks. We first
present a translation from debates to transition systems. Such transition
systems can model debates and represent their evolution over time using a
finite set of states. We then formalise relevant debate properties using
temporal and strategy logics. These formalisations, along with a debate
transition system, allow us to verify whether a given debate satisfies certain
properties. The verification process can be automated using model checkers.
Therefore, we also measure their performance when verifying debates, and use
the results to discuss the feasibility of model checking debates.
"
Reliable Wide-Area Backscatter via Channel Polarization. ,http://arxiv.org/abs/1912.05829,"A long-standing vision of backscatter communications is to provide long-range
connectivity and high-speed transmissions for batteryless Internet-of-Things
(IoT). Recent years have seen major innovations in designing backscatters
toward this goal. Yet, they either operate at a very short range, or experience
extremely low throughput. This paper takes one step further toward breaking
this stalemate, by presenting PolarScatter that exploits channel polarization
in long-range backscatter links. We transform backscatter channels into nearly
noiseless virtual channels through channel polarization, and convey bits with
extremely low error probability. Specifically, we propose a new polar code
scheme that automatically adapts itself to different channel quality, and
design a low-cost encoder to accommodate polar codes on resource-constrained
backscatter tags. We build a prototype PCB tag and test it in various outdoor
and indoor environments. Our experiments show that our prototype achieves up to
10$\times$ throughput gain, or extends the range limit by 1.8$\times$ compared
with the state-of-the-art long-range backscatter solution. We also simulate an
IC design in TSMC 65 nm LP CMOS process. Compared with traditional encoders,
our encoder reduces storage overhead by three orders of magnitude, and lowers
the power consumption to tens of microwatts.
"
Provably Efficient Exploration in Policy Optimization. ,http://arxiv.org/abs/1912.05830,"While policy-based reinforcement learning (RL) achieves tremendous successes
in practice, it is significantly less understood in theory, especially compared
with value-based RL. In particular, it remains elusive how to design a provably
efficient policy optimization algorithm that incorporates exploration. To
bridge such a gap, this paper proposes an Optimistic variant of the Proximal
Policy Optimization algorithm (OPPO), which follows an ""optimistic version"" of
the policy gradient direction. This paper proves that, in the problem of
episodic Markov decision process with linear function approximation, unknown
transition, and adversarial reward with full-information feedback, OPPO
achieves $\tilde{O}(\sqrt{d^3 H^3 T})$ regret. Here $d$ is the feature
dimension, $H$ is the episode horizon, and $T$ is the total number of steps. To
the best of our knowledge, OPPO is the first provably efficient policy
optimization algorithm that explores.
"
STEERAGE: Synthesis of Neural Networks Using Architecture Search and Grow-and-Prune Methods. ,http://arxiv.org/abs/1912.05831,"Neural networks (NNs) have been successfully deployed in many applications.
However, architectural design of these models is still a challenging problem.
Moreover, neural networks are known to have a lot of redundancy. This increases
the computational cost of inference and poses an obstacle to deployment on
Internet-of-Thing sensors and edge devices. To address these challenges, we
propose the STEERAGE synthesis methodology. It consists of two complementary
approaches: efficient architecture search, and grow-and-prune NN synthesis. The
first step, covered in a global search module, uses an accuracy predictor to
efficiently navigate the architectural search space. The predictor is built
using boosted decision tree regression, iterative sampling, and efficient
evolutionary search. The second step involves local search. By using various
grow-and-prune methodologies for synthesizing convolutional and feed-forward
NNs, it reduces the network redundancy, while boosting its performance. We have
evaluated STEERAGE performance on various datasets, including MNIST and
CIFAR-10. On MNIST dataset, our CNN architecture achieves an error rate of
0.66%, with 8.6x fewer parameters compared to the LeNet-5 baseline. For the
CIFAR-10 dataset, we used the ResNet architectures as the baseline. Our
STEERAGE-synthesized ResNet-18 has a 2.52% accuracy improvement over the
original ResNet-18, 1.74% over ResNet-101, and 0.16% over ResNet-1001, while
having comparable number of parameters and FLOPs to the original ResNet-18.
This shows that instead of just increasing the number of layers to increase
accuracy, an alternative is to use a better NN architecture with fewer layers.
In addition, STEERAGE achieves an error rate of just 3.86% with a variant of
ResNet architecture with 40 layers. To the best of our knowledge, this is the
highest accuracy obtained by ResNet-based architectures on the CIFAR-10
dataset.
"
Speech-driven facial animation using polynomial fusion of features. ,http://arxiv.org/abs/1912.05833,"Speech-driven facial animation involves using a speech signal to generate
realistic videos of talking faces. Recent deep learning approaches to facial
synthesis rely on extracting low-dimensional representations and concatenating
them, followed by a decoding step of the concatenated vector. This accounts for
only first-order interactions of the features and ignores higher-order
interactions. In this paper we propose a polynomial fusion layer that models
the joint representation of the encodings by a higher-order polynomial, with
the parameters modelled by a tensor decomposition. We demonstrate the the
suitability of this approach through experiments on generated videos evaluated
on a range of metrics on video quality, audiovisual synchronisation and
generation of blinks.
"
Evaluation of Chebyshev polynomials on intervals and application to root finding. ,http://arxiv.org/abs/1912.05843,"In approximation theory, it is standard to approximate functions by
polynomials expressed in the Chebyshev basis. Evaluating a polynomial $f$ of
degree n given in the Chebyshev basis can be done in $O(n)$ arithmetic
operations using the Clenshaw algorithm. Unfortunately, the evaluation of $f$
on an interval $I$ using the Clenshaw algorithm with interval arithmetic
returns an interval of width exponential in $n$. We describe a variant of the
Clenshaw algorithm based on ball arithmetic that returns an interval of width
quadratic in $n$ for an interval of small enough width. As an application, our
variant of the Clenshaw algorithm can be used to design an efficient root
finding algorithm.
"
Local Context Normalization: Revisiting Local Normalization. ,http://arxiv.org/abs/1912.05845,"Normalization layers have been shown to improve convergence in deep neural
networks. In many vision applications the local spatial context of the features
is important, but most common normalization schemes includingGroup
Normalization (GN), Instance Normalization (IN), and Layer Normalization (LN)
normalize over the entire spatial dimension of a feature. This can wash out
important signals and degrade performance. For example, in applications that
use satellite imagery, input images can be arbitrarily large; consequently, it
is nonsensical to normalize over the entire area. Positional Normalization
(PN), on the other hand, only normalizes over a single spatial position at a
time. A natural compromise is to normalize features by local context, while
also taking into account group level information. In this paper, we propose
Local Context Normalization (LCN): a normalization layer where every feature is
normalized based on a window around it and the filters in its group. We propose
an algorithmic solution to make LCN efficient for arbitrary window sizes, even
if every point in the image has a unique window. LCN outperforms its Batch
Normalization (BN), GN, IN, and LN counterparts for object detection, semantic
segmentation, and instance segmentation applications in several benchmark
datasets, while keeping performance independent of the batch size and
facilitating transfer learning.
"
The Benefits of Close-Domain Fine-Tuning for Table Detection in Document Images. ,http://arxiv.org/abs/1912.05846,"A correct localisation of tables in a document is instrumental for
determining their structure and extracting their contents; therefore, table
detection is a key step in table understanding. Nowadays, the most successful
methods for table detection in document images employ deep learning algorithms;
and, particularly, a technique known as fine-tuning. In this context, such a
technique exports the knowledge acquired to detect objects in natural images to
detect tables in document images. However, there is only a vague relation
between natural and document images, and fine-tuning works better when there is
a close relation between the source and target task. In this paper, we show
that it is more beneficial to employ fine-tuning from a closer domain. To this
aim, we train different object detection algorithms (namely, Mask R-CNN,
RetinaNet, SSD and YOLO) using the TableBank dataset (a dataset of images of
academic documents designed for table detection and recognition), and fine-tune
them for several heterogeneous table detection datasets. Using this approach,
we considerably improve the accuracy of the detection models fine-tuned from
natural images (in mean a 17%, and, in the best case, up to a 60%).
"
"EPIC: An Energy-Efficient, High-Performance GPGPU Computing Research Infrastructure. ",http://arxiv.org/abs/1912.05848,"The pursuit of many research questions requires massive computational
resources. State-of-the-art research in physical processes using simulations,
the training of neural networks for deep learning, or the analysis of big data
are all dependent on the availability of sufficient and performant
computational resources. For such research, access to a high-performance
computing infrastructure is indispensable. Many scientific workloads from such
research domains are inherently parallel and can benefit from the data-parallel
architecture of general purpose graphics processing units (GPGPUs). However,
GPGPU resources are scarce at Norway's national infrastructure. EPIC is a GPGPU
enabled computing research infrastructure at NTNU. It enables NTNU's
researchers to perform experiments that otherwise would be impossible, as
time-to-solution would simply take too long.
"
Exploiting Statistical and Structural Features for the Detection of Domain Generation Algorithms. ,http://arxiv.org/abs/1912.05849,"Nowadays, malware campaigns have reached a high level of sophistication,
thanks to the use of cryptography and covert communication channels over
traditional protocols and services. In this regard, a typical approach to evade
botnet identification and takedown mechanisms is the use of domain fluxing
through the use of Domain Generation Algorithms (DGAs). These algorithms
produce an overwhelming amount of domain names that the infected device tries
to communicate with to find the Command and Control server, yet only a small
fragment of them is actually registered. Due to the high number of domain
names, the blacklisting approach is rendered useless. Therefore, the botmaster
may pivot the control dynamically and hinder botnet detection mechanisms. To
counter this problem, many security mechanisms result in solutions that try to
identify domains from a DGA based on the randomness of their name.
</p>
<p>In this work, we explore hard to detect families of DGAs, as they are
constructed to bypass these mechanisms. More precisely, they are based on the
use of dictionaries so the domains seem to be user-generated. Therefore, the
corresponding generated domains pass many filters that look for, e.g. high
entropy strings. To address this challenge, we propose an accurate and
efficient probabilistic approach to detect them. We test and validate the
proposed solution through extensive experiments with a sound dataset containing
all the wordlist-based DGA families that exhibit this behaviour and compare it
with other state-of-the-art methods, practically showing the efficacy and
prevalence of our proposal.
"
PEEPLL: Privacy-Enhanced Event Pseudonymisation with Limited Linkability. ,http://arxiv.org/abs/1912.05861,"Pseudonymisation provides the means to reduce the privacy impact of
monitoring, auditing, intrusion detection, and data collection in general on
individual subjects. Its application on data records, especially in an
environment with additional constraints, like re-identification in the course
of incident response, implies assumptions and privacy issues, which contradict
the achievement of the desirable privacy level. Proceeding from two real-world
scenarios, where personal and identifying data needs to be processed, we
identify requirements as well as a system model for pseudonymisation and
explicitly state the sustained privacy threats, even when pseudonymisation is
applied. With this system and threat model, we derive privacy protection goals
together with possible technical realisations, which are implemented and
integrated into our event pseudonymisation framework PEEPLL for the context of
event processing, like monitoring and auditing of user, process, and network
activities. Our framework provides privacy-friendly linkability in order to
maintain the possibility for automatic event correlation and evaluation, while
at the same time reduces the privacy impact on individuals. Additionally, the
pseudonymisation framework is evaluated in order to provide some restrained
insights on the impact of assigned paradigms and all necessary new mechanisms
on the performance of monitoring and auditing. With this framework, privacy
provided by event pseudonymisation can be enhanced by a more rigorous
commitment to the concept of personal data minimisation, especially in the
context of regulatory requirements like the European General Data Protection
Regulation.
"
Totally Deep Support Vector Machines. ,http://arxiv.org/abs/1912.05864,"Support vector machines (SVMs) have been successful in solving many computer
vision tasks including image and video category recognition especially for
small and mid-scale training problems. The principle of these non-parametric
models is to learn hyperplanes that separate data belonging to different
classes while maximizing their margins. However, SVMs constrain the learned
hyperplanes to lie in the span of support vectors, fixed/taken from training
data, and this reduces their representational power and may lead to limited
generalization performances. In this paper, we relax this constraint and allow
the support vectors to be learned (instead of being fixed/taken from training
data) in order to better fit a given classification task. Our approach,
referred to as deep total variation support vector machines, is parametric and
relies on a novel deep architecture that learns not only the SVM and the kernel
parameters but also the support vectors, resulting into highly effective
classifiers. We also show (under a particular setting of the activation
functions in this deep architecture) that a large class of kernels and their
combinations can be learned. Experiments conducted on the challenging task of
skeleton-based action recognition show the outperformance of our deep total
variation SVMs w.r.t different baselines as well as the related work.
"
